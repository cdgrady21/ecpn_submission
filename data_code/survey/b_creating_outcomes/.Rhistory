##datv2 <- draw_data(thedesign)
##table(datv1$Z,datv2$Z)
plan(sequential)
sims <- 1000
set.seed(12345)
thediagnosis <- diagnose_design(thedesignPlusEstimators, sims = sims, bootstrap_sims = 0)
## See https://haozhu233.github.io/kableExtra/awesome_table_in_html.html
kable(reshape_diagnosis(thediagnosis)[c(3,1,2,4,5),diagcols] ) # %>% kable_styling() %>% scroll_box(width = "100%", height = "600px")
25*10
250*12
21*81
217/14371
.01^30
10000*30
(2^30)/100
(1+2^29)/100
((1+2^29)/100) - (10000*30)
(((1+2^29)/100)*2) - (10000*30)
50*.25
.03*1806
2000000000*.01
10/1600
10/850
install.packages("rmarkdown")
install.packages("qdap")
text <- "Text mining usually involves the process of structuring the input text. The overarching goal is, essentially, to turn text into data for analysis, via application of natural language processing (NLP) and analytical methods."
frequent_terms <- freq_terms(text, 3)
library(qdap)
frequent_terms <- freq_terms(text, 3)
frequent_terms <- qdap::freq_terms(text, 3)
1+9+3+8
21/4
200*80
20*360
28*360
30000/160
500*160
300*160
200*30
300*30
300*20
6000*12
30*18*28
30*18*28*3
160/3
300*55
300*160
330*6
1980/20
330/20
# 20 enumerators,
test <- 1:6
?sample
# 20 enumerators,
test <- sample(1:6, size=20, replace=T)
test
?rep
x <- rep(sample(1:6, size=20, replace=T),20)
x
once <- mosaic::do(20)*(sample(1:6, size=20, replace=T))
once
# 20 enumerators,
test.fun <- function()
{
once <- mosaic::do(20)*(sample(1:6, size=20, replace=T))
}
dists <- mosaic::do(1000)*test.fun()
summary(dists$V1)
dists[1:5,1:5]
dim(dists)
dists <- mosaic::do(1000)*(sample(1:6, size=20, replace=T))
dim(dists)
sample(1:6, size=100, replace=T)
# 20 enumerators = 100 svys each, sorted into 6 groups
dists <- mosaic::do(1000)*(sample(1:6, size=100, replace=T))
dim(dists)
summary(dists[,1;10])
summary(dists[,1:10])
summary(dists[1:20,1:10])
335*6
2000/20
100/6
mode(dists[1:20,1:10])
Modes <- function(x) {
ux <- unique(x)
tab <- tabulate(match(x, ux))
ux[tab == max(tab)]
}
Modes(dists[1:20,1:10])
Modes(dists[1,1:10])
Modes(dists[1:20,1])
Modes(dists[1:20,2])
Modes(dists[1:20,3])
8^6
(8^6)*4
2^9
1316/4
(1316/4)/7
20*700
2020-63
1989+16
13927-6300
10000*200
90/7
1200*300000000
n=3+4+3+3+5+14+31+44+43+39+30+27+24+18+15+11+9+8+5+4+1
n
341/21
80-(11.96*3)
11.96*3
36+59
63*.77
400000*1.03
400000*1.33
10000*750
10000/(137000+120000)
20000*1.1
20000*.1
2000*.65
100000/725
100000/750
15/1000
100*(2.89)^25
100*(2.89^25)
100*2.89
100*1.0289
100*(1.0289^25)
100*(1.0289^24)
.09*.09
1-(.09*.09)
133/176
152/215
setwd("C:/Users/cdgra/Google Drive/GradSchool/Dissertation/Writing/LabExp/replication_2_lucid/survey")
# Chunk 1: setup
knitr::opts_chunk$set(echo = TRUE)
rm(list=ls())
load("../../analysis/lab_df_image.Rdata")
load("../../replication/analysis/final_2020_df_image.Rdata")
# Chunk 2
outcomes <- c("ftherm_diff", "affect_cum_diff", "dist_diff",
"resp_prob", "threat_diff", "perc_diff",
"group_diff", "group_diff_01")
#all go from -1 to +1
## check that no outcomes less than -1 or higher than 1
stopifnot(dplyr::between(range(df[,outcomes],na.rm=T),-1,1))
# Chunk 3
## from cyrus samii IC weighting
# Function to standardize columns of a matrix
# where you designate a standardization group
# (e.g., the control group in an experiment)
# with "sgroup", a logical vector.
matStand <- function(x, sgroup = rep(TRUE, nrow(x))){
for(j in 1:ncol(x)){
x[,j] <- (x[,j] - mean(x[sgroup,j]))/sd(x[sgroup,j]) # chris: demeans outcome, then divides by sd
}
return(x)
}
# Function that takes in data in matrix format and returns
# (i) IC weights and (ii) ICW index.
# Weights can be incorporated using the "wgts" argument.
# The "revcols" argument takes a vector indicating which columns,
# if any, should have values reversed (that is, standardized
# values are multiplied by -1) prior to construction of the index.
icwIndex <- function(	xmat,
wgts=rep(1, nrow(xmat)),
revcols = NULL,
sgroup = rep(TRUE, nrow(xmat))){
X <- matStand(xmat, sgroup)
if(length(revcols)>0){
X[,revcols] <-  -1*X[,revcols]
}
i.vec <- as.matrix(rep(1,ncol(xmat)))
Sx <- cov.wt(X, wt=wgts)[[1]]
weights <- solve(t(i.vec)%*%solve(Sx)%*%i.vec)%*%t(i.vec)%*%solve(Sx)
index <- t(solve(t(i.vec)%*%solve(Sx)%*%i.vec)%*%t(i.vec)%*%solve(Sx)%*%t(X))
return(list(weights = weights, index = index))
}
# Chunk 4
# resp_prob initially is 1 for good response 0 for bad response
df$resp_prob <- (df$resp_prob-max(df$resp_prob))*-1
# threat_diff initially is low when outgroup more threatening, high when ingroup more threatening
df$threat_diff <- df$threat_diff*-1
# Chunk 5
# range
df[,paste0(outcomes, "_range" )] <- lapply(df[,outcomes], reshape::rescaler, type="range")
#SD
df[,paste0(outcomes, "_sd" )] <- lapply(df[,outcomes], reshape::rescaler, type="sd")
# Chunk 6
# without social distance
df$affect_index <- rowMeans(df[,c("ftherm_diff",
"affect_cum_diff")])
df$affect_index_range <- rowMeans(df[,c("ftherm_diff_range",
"affect_cum_diff_range")])
df$affect_index_sd <- rowMeans(df[,c("ftherm_diff_sd",
"affect_cum_diff_sd")])
#df$affect_index2 <- rowMeans(sapply(df[,c("ftherm_diff","affect_cum_diff")], reshape::rescaler, type="sd"))
# Chunk 7
# supposed to get covariances only from control group; need a vector 1 for CO, 0 otherwise
df$control <- ifelse(df$treatment %in% "co", 1, 0)
# on all subjects
df$control <- 1
ic.fun <- function(vars, thedf)
{
x <- thedf[,vars]
x <- zoo::na.aggregate(x)
out_cw <- icwIndex(xmat=x, sgroup=as.logical(thedf[,'control']))
print(cor(x))
print(out_cw$weights)
return(as.vector(out_cw$index))
}
# from the correlation matrix
fit <- princomp(df[,rep_affect_pos], cor=TRUE)
summary(fit) # print variance accounted for
c(rep_affect_pos, rep_affect_neg)
# from the correlation matrix
fit <- princomp(df[,c(rep_affect_pos, rep_affect_neg)], cor=TRUE)
summary(fit) # print variance accounted for
loadings(fit) # pc loadings
plot(fit,type="lines") # scree plot
fit$scores # the principal components
biplot(fit)
source("http://www.stat.sc.edu/~habing/courses/530/fact.txt")
rm(list=ls())
source("http://www.stat.sc.edu/~habing/courses/530/fact.txt")
lambda <- fact(x,method="iter",maxfactors=1, niter=130)$loadings
lambda <- fact(df[,rep_affect_pos],method="iter",maxfactors=1, niter=130)$loadings
?fact
fact
lambda <- fact(x=df[,rep_affect_pos],method="iter",maxfactors=1, niter=130)$loadings
ev1 <- eigen(df[,rep_affect_pos]) # get eigenvalues
rep_affect_pos
# Chunk 1: setup
knitr::opts_chunk$set(echo = TRUE)
rm(list=ls())
load("../../analysis/lab_df_image.Rdata")
load("../../replication/analysis/final_2020_df_image.Rdata")
# Chunk 2
outcomes <- c("ftherm_diff", "affect_cum_diff", "dist_diff",
"resp_prob", "threat_diff", "perc_diff",
"group_diff", "group_diff_01")
#all go from -1 to +1
## check that no outcomes less than -1 or higher than 1
stopifnot(dplyr::between(range(df[,outcomes],na.rm=T),-1,1))
# Chunk 3
## from cyrus samii IC weighting
# Function to standardize columns of a matrix
# where you designate a standardization group
# (e.g., the control group in an experiment)
# with "sgroup", a logical vector.
matStand <- function(x, sgroup = rep(TRUE, nrow(x))){
for(j in 1:ncol(x)){
x[,j] <- (x[,j] - mean(x[sgroup,j]))/sd(x[sgroup,j]) # chris: demeans outcome, then divides by sd
}
return(x)
}
# Function that takes in data in matrix format and returns
# (i) IC weights and (ii) ICW index.
# Weights can be incorporated using the "wgts" argument.
# The "revcols" argument takes a vector indicating which columns,
# if any, should have values reversed (that is, standardized
# values are multiplied by -1) prior to construction of the index.
icwIndex <- function(	xmat,
wgts=rep(1, nrow(xmat)),
revcols = NULL,
sgroup = rep(TRUE, nrow(xmat))){
X <- matStand(xmat, sgroup)
if(length(revcols)>0){
X[,revcols] <-  -1*X[,revcols]
}
i.vec <- as.matrix(rep(1,ncol(xmat)))
Sx <- cov.wt(X, wt=wgts)[[1]]
weights <- solve(t(i.vec)%*%solve(Sx)%*%i.vec)%*%t(i.vec)%*%solve(Sx)
index <- t(solve(t(i.vec)%*%solve(Sx)%*%i.vec)%*%t(i.vec)%*%solve(Sx)%*%t(X))
return(list(weights = weights, index = index))
}
# Chunk 4
# resp_prob initially is 1 for good response 0 for bad response
df$resp_prob <- (df$resp_prob-max(df$resp_prob))*-1
# threat_diff initially is low when outgroup more threatening, high when ingroup more threatening
df$threat_diff <- df$threat_diff*-1
# Chunk 5
# range
df[,paste0(outcomes, "_range" )] <- lapply(df[,outcomes], reshape::rescaler, type="range")
#SD
df[,paste0(outcomes, "_sd" )] <- lapply(df[,outcomes], reshape::rescaler, type="sd")
# Chunk 6
# without social distance
df$affect_index <- rowMeans(df[,c("ftherm_diff",
"affect_cum_diff")])
df$affect_index_range <- rowMeans(df[,c("ftherm_diff_range",
"affect_cum_diff_range")])
df$affect_index_sd <- rowMeans(df[,c("ftherm_diff_sd",
"affect_cum_diff_sd")])
#df$affect_index2 <- rowMeans(sapply(df[,c("ftherm_diff","affect_cum_diff")], reshape::rescaler, type="sd"))
# Chunk 7
# supposed to get covariances only from control group; need a vector 1 for CO, 0 otherwise
df$control <- ifelse(df$treatment %in% "co", 1, 0)
# on all subjects
df$control <- 1
ic.fun <- function(vars, thedf)
{
x <- thedf[,vars]
x <- zoo::na.aggregate(x)
out_cw <- icwIndex(xmat=x, sgroup=as.logical(thedf[,'control']))
print(cor(x))
print(out_cw$weights)
return(as.vector(out_cw$index))
}
# thompson method
source("http://www.stat.sc.edu/~habing/courses/530/fact.txt")
lambda <- fact(x=df[,rep_affect_pos],method="iter",maxfactors=1, niter=130)$loadings
ev1 <- eigen(df[,rep_affect_pos]) # get eigenvalues
# Determine Number of Factors to Extract
library(nFactors)
ev1 <- eigen(df[,rep_affect_pos]) # get eigenvalues
sum(is.na(df))
summary(df[,rep_affect_pos])
ev1 <- eigen(df[,rep_affect_pos]) # get eigenvalues
df[,rep_affect_pos]
ev1 <- eigen(df[,rep_affect_pos]) # get eigenvalues
?nFactors::eigen
??eigen
df[,rep_affect_pos]
ev1 <- eigen(df) # get eigenvalues
rep_affect_pos
ev1 <- eigen(df[,c("rep_respectful", "rep_happy"]) # get eigenvalues
ev1 <- eigen(df[,c("rep_respectful", "rep_happy")]) # get eigenvalues
ev1 <- eigen(cor(df[,rep_affect_pos])) # get eigenvalues
ap1 <- parallel(subject=nrow(df[,rep_affect_pos]),var=ncol(df[,rep_affect_pos]),
rep=100,cent=.05)
nS1 <- nScree(x=ev1$values, aparallel=ap1$eigen$qevpea)
plotnScree(nS1)
ev1 <- eigen(cor(df[,c(rep_affect_pos, rep_affect_neg)])) # get eigenvalues
ap1 <- parallel(subject=nrow(df[,rep_affect_pos]),var=ncol(df[,rep_affect_pos]),
rep=100,cent=.05)
nS1 <- nScree(x=ev1$values, aparallel=ap1$eigen$qevpea)
plotnScree(nS1)
summary(df[,c(rep_affect_pos, rep_affect_neg)])
ev1 <- eigen(cor(df[,c(rep_affect_pos, rep_affect_neg)])) # get eigenvalues
ap1 <- parallel(subject=nrow(df[,c(rep_affect_pos, rep_affect_neg)]),
var=ncol(df[,c(rep_affect_pos, rep_affect_neg)]),
rep=100,cent=.05)
nS1 <- nScree(x=ev1$values, aparallel=ap1$eigen$qevpea)
plotnScree(nS1)
# from the correlation matrix
fit <- princomp(df[,c(rep_affect_pos, rep_affect_neg)], cor=TRUE)
summary(fit) # print variance accounted for
?fit
??fit
?princomp
fit <- factanal(df[,c(rep_affect_pos, rep_affect_neg)],
2, rotation="varimax")
fit
print(fit, digits=2, cutoff=.3, sort=TRUE)
fit <- factanal(df[,c(rep_affect_pos, rep_affect_neg)],
3, rotation="varimax")
print(fit, digits=2, cutoff=.3, sort=TRUE)
# plot factor 1 by factor 2
load <- fit$loadings[,1:2]
plot(load,type="n") # set up plot
text(load,labels=names(mydata),cex=.7) # add variable names
text(load,labels=names(df[,c(rep_affect_pos, rep_affect_neg)]),cex=.7) # add variable names
# from the correlation matrix
fit <- princomp(df[,c(rep_affect_pos, rep_affect_neg)], cor=TRUE)
summary(fit) # print variance accounted for
loadings(fit) # pc loadings
plot(fit,type="lines") # scree plot
fit$scores # the principal components
biplot(fit)
fit$scores # the principal components
loadings(fit) # pc loadings
# Towards Republiacns
ev1 <- eigen(cor(df[,c(rep_affect_pos, rep_affect_neg)])) # get eigenvalues
ap1 <- parallel(subject=nrow(df[,c(rep_affect_pos, rep_affect_neg)]),
var=ncol(df[,c(rep_affect_pos, rep_affect_neg)]),
rep=100,cent=.05)
nS1 <- nScree(x=ev1$values, aparallel=ap1$eigen$qevpea)
plotnScree(nS1)
# 2 factors
fit <- factanal(df[,c(rep_affect_pos, rep_affect_neg)],
2, rotation="varimax")
print(fit, digits=2, cutoff=.3, sort=TRUE)
print(fit, digits=2, cutoff=.3, sort=TRUE)
# 2 factors
fit <- factanal(df[,c(rep_affect_pos, rep_affect_neg)],
1, rotation="varimax")
print(fit, digits=2, cutoff=.3, sort=TRUE)
# 2 factors
fit <- factanal(df[,c(rep_affect_pos, rep_affect_neg)],
3, rotation="varimax")
print(fit, digits=2, cutoff=.3, sort=TRUE)
# 2 factors
fit <- factanal(df[,c(rep_affect_pos, rep_affect_neg)],
2, rotation="varimax")
print(fit, digits=2, cutoff=.3, sort=TRUE)
plot(load,type="n") # set up plot
text(load,labels=names(df[,c(rep_affect_pos, rep_affect_neg)]),cex=.7) # add variable names
# Towards Republiacns
ev1 <- eigen(cor(df[,rep_affect_pos])) # get eigenvalues
ap1 <- parallel(subject=nrow(df[,rep_affect_pos]),
var=ncol(df[,rep_affect_pos]),
rep=100,cent=.05)
nS1 <- nScree(x=ev1$values, aparallel=ap1$eigen$qevpea)
plotnScree(nS1)
# 2 factors
fit <- factanal(df[,c(rep_affect_pos, rep_affect_neg)],
2, rotation="varimax")
print(fit, digits=2, cutoff=.3, sort=TRUE)
# 2 factors
fit <- factanal(df[,rep_affect_pos],
2, rotation="varimax")
print(fit, digits=2, cutoff=.3, sort=TRUE)
load <- fit$loadings[,1:2]
plot(load,type="n") # set up plot
text(load,labels=names(df[,c(rep_affect_pos, rep_affect_neg)]),cex=.7) # add variable names
# 1 factors
fit <- factanal(df[,rep_affect_pos],
1, rotation="varimax")
print(fit, digits=2, cutoff=.3, sort=TRUE)
load <- fit$loadings[,1:2]
plot(load,type="n") # set up plot
text(load,labels=names(df[,c(rep_affect_pos, rep_affect_neg)]),cex=.7) # add variable names
# Chunk 1: setup
knitr::opts_chunk$set(echo = TRUE)
rm(list=ls())
load("../../analysis/lab_df_image.Rdata")
load("../../replication/analysis/final_2020_df_image.Rdata")
# packages
library(nFactors)
# Chunk 2
# General Stuff: Vector of outcomes names
outcomes <- c("ftherm_diff", "affect_cum_diff", "dist_diff",
"resp_prob", "threat_diff", "perc_diff",
"group_diff", "group_diff_01")
#all go from -1 to +1
## check that no outcomes less than -1 or higher than 1
stopifnot(dplyr::between(range(df[,outcomes],na.rm=T),-1,1))
# Function from Cyrus Samii for IC weighting and factor analysis.
# Function to standardize columns of a matrix
# where you designate a standardization group
# (e.g., the control group in an experiment)
# with "sgroup", a logical vector.
matStand <- function(x, sgroup = rep(TRUE, nrow(x))){
for(j in 1:ncol(x)){
x[,j] <- (x[,j] - mean(x[sgroup,j]))/sd(x[sgroup,j]) # chris: demeans outcome, then divides by sd
}
return(x)
}
# Function that takes in data in matrix format and returns
# (i) IC weights and (ii) ICW index.
# Weights can be incorporated using the "wgts" argument.
# The "revcols" argument takes a vector indicating which columns,
# if any, should have values reversed (that is, standardized
# values are multiplied by -1) prior to construction of the index.
icwIndex <- function(	xmat,
wgts=rep(1, nrow(xmat)),
revcols = NULL,
sgroup = rep(TRUE, nrow(xmat))){
X <- matStand(xmat, sgroup)
if(length(revcols)>0){
X[,revcols] <-  -1*X[,revcols]
}
i.vec <- as.matrix(rep(1,ncol(xmat)))
Sx <- cov.wt(X, wt=wgts)[[1]]
weights <- solve(t(i.vec)%*%solve(Sx)%*%i.vec)%*%t(i.vec)%*%solve(Sx)
index <- t(solve(t(i.vec)%*%solve(Sx)%*%i.vec)%*%t(i.vec)%*%solve(Sx)%*%t(X))
return(list(weights = weights, index = index))
}
# Chunk 3
# Cleaning: Make Outcomes Same Valence.
## Need high scores BAD for every variable.
#ftherm difference.  bigger number == more distance between ingroup/outgroup == bad
#affect_cum_diff: bigger numbers == more favoritism towards ingroup == bad
#dist_diff: high numbers == more distance == bad.
#**resp_prob**: high numbers == more likely to say BOTH responsible == good!
#**threat_diff**: neg numbers == more threat from outgroup relative to ingroup == bad.
#perc_diff: high numbers == willingness to live/join ingroup but not outgroup == BAD
#group diff: higher numbers == more preference for ingroup over outgroup == bad
# resp_prob initially is 1 for good response 0 for bad response
df$resp_prob <- (df$resp_prob-max(df$resp_prob))*-1
# threat_diff initially is low when outgroup more threatening, high when ingroup more threatening
df$threat_diff <- df$threat_diff*-1
# Make outcomes same scale
##Right now every outcome is on a -1 to +1 scale, where -1 means bias towards OUTGROUP, +1 means bias towards INGROUP, 0 means no bias.
##Now doing a range (0-1) rescale, where high scores indicate more favoritism towards ingroup.  I actually prefer my -1 to +1.
##And SD rescale where lower numbers are better / higher numbers are worse.
# range
df[,paste0(outcomes, "_range" )] <- lapply(df[,outcomes], reshape::rescaler, type="range")
#SD
df[,paste0(outcomes, "_sd" )] <- lapply(df[,outcomes], reshape::rescaler, type="sd")
# Chunk 4
# supposed to get covariances only from control group; need a vector 1 for CO, 0 otherwise
df$control <- ifelse(df$treatment %in% "co", 1, 0)
# on all subjects
df$control <- 1
ic.fun <- function(vars, thedf)
{
x <- thedf[,vars]
x <- zoo::na.aggregate(x)
out_cw <- icwIndex(xmat=x, sgroup=as.logical(thedf[,'control']))
print(cor(x))
print(out_cw$weights)
return(as.vector(out_cw$index))
}
setwd("C:/Users/cdgra/Google Drive/Africa/Nigeria - Farmer-Pastoralist/ecpn_work/analysis/academicPaper/survey/b_creating_outcomes")
library(estimatr)
library(dplyr)
rm(list=ls())
load("../a_cleaning/c_dataCombine.Rdata")
table(base$income_month)
summary(base$income_month)
summary(end$income_month)
